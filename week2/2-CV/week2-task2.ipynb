{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226c4bd0-2726-4674-b7db-4c1704f4c7ca",
   "metadata": {},
   "source": [
    "# CLIP推理（效果不好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1e1c5d-a369-44de-83c1-6728be38e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\tech\\极客时间\\训练营\\A7-多模态大模型训练营2599\\作业\\MultiModelTrain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1206 22:19:11.752000 5940 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.9916e-01, 8.4369e-04]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "model = CLIPModel.from_pretrained(\"C:/Users/LiuFeng/.cache/huggingface/hub/models--openai--clip-vit-base-patch16/snapshots/57c216476eefef5ab752ec549e440a49ae4ae5f3\")\n",
    "processor = CLIPProcessor.from_pretrained(\"C:/Users/LiuFeng/.cache/huggingface/hub/models--openai--clip-vit-base-patch16/snapshots/57c216476eefef5ab752ec549e440a49ae4ae5f3\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8144b632-1ed3-4a6e-8d1f-bca88474e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"data/re_id_1000_test.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "733fae52-c012-4728-a5e8-a0898ce7e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Toyota_Camry\", \"Toyota_Corolla\", \"Toyota_Corolla_EX\", \"Buick_LaCrosse\", \"Volkswagen_Magotan\", \"Audi_A4\",\n",
    "               \"Nissan_Sylphy\", \"Nissan_Tiida\", \"Honda_Accord\", \"Ford_Focus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de877b22-62c5-4cac-a23d-cbd45c679eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1830, 0.1186, 0.1731, 0.0035, 0.0034, 0.0037, 0.3545, 0.1169, 0.0231,\n",
       "         0.0201]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(\"data/image/\" + test[0].values[100].replace(\"\\\\\", \"/\"))\n",
    "inputs = processor(text=[\"a photo of a \" + x for x in class_names], images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "logits_per_image.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74108153-61a3-4774-889d-abc7ddfe4c3d",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dae8ac2-67c0-433f-aa49-fb20cc49aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\tech\\极客时间\\训练营\\A7-多模态大模型训练营2599\\作业\\MultiModelTrain\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1206 22:50:52.173000 24712 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms, datasets\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "793b38b3-e213-4d3a-b7b4-182d06a85f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"data/re_id_1000_test.txt\", header=None)\n",
    "train = pd.read_csv(\"data/re_id_1000_train.txt\", header=None)\n",
    "\n",
    "test[\"label\"] = test[0].apply(lambda x: int(x.split(\"\\\\\")[0]) - 1)\n",
    "test[\"path\"] = test[0].apply(lambda x: \"data/image/\" + x.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "train[\"label\"] = train[0].apply(lambda x: int(x.split(\"\\\\\")[0]) - 1)\n",
    "train[\"path\"] = train[0].apply(lambda x: \"data/image/\" + x.replace(\"\\\\\", \"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698c9fa0-d800-42f1-ad59-ff7affcb29a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        object\n",
       "label     int64\n",
       "path     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7574c035-3081-44a5-9320-4e8c7a450413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1\\License_1\\4404000000002940408492.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>data/image/1/License_1/4404000000002940408492.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1\\License_1\\4404000000002949435646.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>data/image/1/License_1/4404000000002949435646.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1\\License_1\\4404000000002949812715.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>data/image/1/License_1/4404000000002949812715.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        0  label  \\\n",
       "0  1\\License_1\\4404000000002940408492.jpg      0   \n",
       "1  1\\License_1\\4404000000002949435646.jpg      0   \n",
       "2  1\\License_1\\4404000000002949812715.jpg      0   \n",
       "\n",
       "                                                path  \n",
       "0  data/image/1/License_1/4404000000002940408492.jpg  \n",
       "1  data/image/1/License_1/4404000000002949435646.jpg  \n",
       "2  data/image/1/License_1/4404000000002949812715.jpg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b496932-d66a-4f01-b599-568af09be6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, unique_labels, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.unique_labels = unique_labels\n",
    "        self.label_to_index = {label: i for i, label in enumerate(self.unique_labels)}\n",
    "        self.num_classes = len(self.label_to_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['path']\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Image not found at {img_path}. Returning a dummy sample.\")\n",
    "            return torch.zeros(3, 224, 224), 0\n",
    "\n",
    "        label_name = self.dataframe.iloc[idx]['label']\n",
    "        label_index = self.label_to_index[label_name]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {'pixel_values': image, 'labels': label_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "064a5108-e703-4e58-a961-6690b6ef75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(256),         # Resize shortest side to 256\n",
    "        transforms.CenterCrop(224),     # Center crop to 224x224 (ResNet input size)\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        logits = self.model(pixel_values)\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            return (loss, logits)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ✅ 用 evaluate 替代 load_metric\n",
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff01dbac-b7c9-4660-ae90-4fc641f031fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = get_data_transforms()\n",
    "\n",
    "train_set = CustomImageDataset(train, train[\"label\"].unique(), transform)\n",
    "test_set = CustomImageDataset(test, train[\"label\"].unique(), transform)\n",
    "\n",
    "train_len = int(0.9 * len(train_set))\n",
    "val_len = len(train_set) - train_len\n",
    "train_set, val_set = random_split(train_set, [train_len, val_len])\n",
    "\n",
    "num_classes = 10 \n",
    "model = ResNetClassifier(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae08eac-3c6f-4524-950e-0759c3413fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \n",
      "  0%|          | 0/71 [04:20<?, ?it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3393, 'grad_norm': 1.3046152591705322, 'learning_rate': 5e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                      \n",
      "\n",
      "\u001b[A\u001b[A                                       \n",
      "  0%|          | 0/71 [05:22<?, ?it/s]         \n",
      "\u001b[A\n",
      "                                      \n",
      "100%|██████████| 71/71 [03:15<00:00,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009686819277703762, 'eval_accuracy': 1.0, 'eval_runtime': 10.6867, 'eval_samples_per_second': 46.787, 'eval_steps_per_second': 0.749, 'epoch': 1.0}\n",
      "{'train_runtime': 195.2534, 'train_samples_per_second': 23.047, 'train_steps_per_second': 0.364, 'train_loss': 0.9618983789229058, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=71, training_loss=0.9618983789229058, metrics={'train_runtime': 195.2534, 'train_samples_per_second': 23.047, 'train_steps_per_second': 0.364, 'total_flos': 0.0, 'train_loss': 0.9618983789229058, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                          # Output directory for checkpoints and logs\n",
    "    num_train_epochs=1,                              # Total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,          # Batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,           # Batch size for evaluation\n",
    "    warmup_steps=500,                                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                               # Strength of weight decay\n",
    "    logging_dir='./logs',                            # Directory for storing logs\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",                     # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",                           # Save checkpoint after each epoch\n",
    "    load_best_model_at_end=True,                     # Load the best model at the end\n",
    "    metric_for_best_model=\"accuracy\",                # Use accuracy to determine best model\n",
    "    learning_rate=5e-4,                              # Starting learning rate\n",
    "    remove_unused_columns=False,                     # Keep all columns in the dataset for flexibility\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                                     # The instantiated model\n",
    "    args=training_args,                              # Training arguments\n",
    "    train_dataset=train_set,                         # Training dataset\n",
    "    eval_dataset=val_set,                            # Validation dataset\n",
    "    compute_metrics=compute_metrics,                 # Function to compute evaluation metrics\n",
    "    data_collator=custom_collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35543498-a6d3-42b1-8b3c-18d324e87c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:51<00:00,  1.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.015545063652098179,\n",
       " 'eval_accuracy': 0.999,\n",
       " 'eval_runtime': 113.5685,\n",
       " 'eval_samples_per_second': 44.026,\n",
       " 'eval_steps_per_second': 0.696,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = trainer.evaluate(test_set)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc56215-97a4-41a3-b080-cc09c87e5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, transform, label_names):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = transform(image)\n",
    "        input_batch = input_tensor.unsqueeze(0) \n",
    "\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "        input_batch = input_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        probabilities = probabilities[0]\n",
    "        predicted_index = torch.argmax(probabilities).item()\n",
    "        predicted_label = label_names[predicted_index]\n",
    "        confidence = probabilities[predicted_index].item() * 100\n",
    "\n",
    "        print(f\"Predicted Class Index: {predicted_index}\")\n",
    "        print(f\"Predicted Label: {predicted_label}\")\n",
    "        print(f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Image file not found at path: {image_path}\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"An unexpected error occurred during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c41683c-33e4-41dd-be31-3b4cc12d597d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>4\\License_301\\4404000000002941735050.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>data/image/4/License_301/440400000000294173505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>6\\License_518\\4404000000002942172377.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>data/image/6/License_518/440400000000294217237...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>6\\License_531\\4404000000002947342645.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>data/image/6/License_531/440400000000294734264...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0  label  \\\n",
       "1501  4\\License_301\\4404000000002941735050.jpg      3   \n",
       "2586  6\\License_518\\4404000000002942172377.jpg      5   \n",
       "2653  6\\License_531\\4404000000002947342645.jpg      5   \n",
       "\n",
       "                                                   path  \n",
       "1501  data/image/4/License_301/440400000000294173505...  \n",
       "2586  data/image/6/License_518/440400000000294217237...  \n",
       "2653  data/image/6/License_531/440400000000294734264...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49a434bd-3024-4eed-a2b8-911965599f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Index: 0\n",
      "Predicted Label: Toyota_Camry\n",
      "Confidence: 99.73%\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"Toyota_Camry\", \"Toyota_Corolla\", \"Toyota_Corolla_EX\", \"Buick_LaCrosse\", \"Volkswagen_Magotan\", \"Audi_A4\",\n",
    "               \"Nissan_Sylphy\", \"Nissan_Tiida\", \"Honda_Accord\", \"Ford_Focus\"]\n",
    "predict_single_image(trainer.model, \"data/image/1/License_20/4404000000002940725419.jpg\", transform, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "060fa5ad-699f-4644-914d-2f8fdb6a9027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Index: 5\n",
      "Predicted Label: Audi_A4\n",
      "Confidence: 99.45%\n"
     ]
    }
   ],
   "source": [
    "class_names = [\"Toyota_Camry\", \"Toyota_Corolla\", \"Toyota_Corolla_EX\", \"Buick_LaCrosse\", \"Volkswagen_Magotan\", \"Audi_A4\",\n",
    "               \"Nissan_Sylphy\", \"Nissan_Tiida\", \"Honda_Accord\", \"Ford_Focus\"]\n",
    "predict_single_image(trainer.model, \"data/image/6/License_531/4404000000002947607633.jpg\", transform, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71c33f13-5990-4a95-a316-bfd4726e21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
